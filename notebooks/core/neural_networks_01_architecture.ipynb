{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "558b9f87",
   "metadata": {},
   "source": [
    "# Neural Networks tutorial 01\n",
    "## **Architecture**\n",
    "\n",
    "Showcasing some of the neural network architectures available in quantEM, how to initialize them, and how inputs/outputs work. \n",
    "\n",
    "Demos of how to train/use the networks are in subsequent notebooks. \n",
    "\n",
    "Arthur McCray  \n",
    "Jan 6, 2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac4a91ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3175c3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amccray/code/quantem/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantEM version: 0.1.7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from quantem.core.ml import DenseNN, CNN2d, CNN3d, CNNDense, HSiren, ConvAutoencoder2d\n",
    "from torchinfo import summary\n",
    "\n",
    "from importlib.metadata import version # Temporary fix for version check\n",
    "print(f\"quantEM version: {version(\"quantem\")}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81f3252",
   "metadata": {},
   "source": [
    "# Dense NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc70d66",
   "metadata": {},
   "source": [
    "Fully-connected, i.e. dense NN\n",
    "- 1D input data to 1D output data\n",
    "    - `(BATCH_SIZE, input_dim)` -> `(BATCH_SIZE, output_dim)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f02a647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: <class 'quantem.core.ml.dense_nn.DenseNN'>\n",
      "Input shape (batch_size, input_dim): torch.Size([5, 10]) -> Output shape (batch_size, output_dim): torch.Size([5, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DenseNN                                  [5, 4]                    --\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─Sequential: 2-1                   [5, 128]                  --\n",
       "│    │    └─Linear: 3-1                  [5, 128]                  1,408\n",
       "│    │    └─Complex_ReLU: 3-2            [5, 128]                  --\n",
       "│    └─Sequential: 2-2                   [5, 128]                  --\n",
       "│    │    └─Linear: 3-3                  [5, 128]                  16,512\n",
       "│    │    └─Complex_ReLU: 3-4            [5, 128]                  --\n",
       "│    └─Sequential: 2-3                   [5, 128]                  --\n",
       "│    │    └─Linear: 3-5                  [5, 128]                  16,512\n",
       "│    │    └─Complex_ReLU: 3-6            [5, 128]                  --\n",
       "│    └─Linear: 2-4                       [5, 4]                    516\n",
       "├─Identity: 1-2                          [5, 4]                    --\n",
       "==========================================================================================\n",
       "Total params: 34,948\n",
       "Trainable params: 34,948\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.17\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.03\n",
       "Params size (MB): 0.28\n",
       "Estimated Total Size (MB): 0.31\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 10\n",
    "output_dim = 4\n",
    "activation = \"relu\" # Activation function for hidden layers, default \"relu\"\n",
    "dtype = torch.complex64 # default torch.float32\n",
    "\n",
    "## model architecture can be specified in 2 ways, by declaring the size of each layer individually:\n",
    "hidden_dims = [128, 128, 128] \n",
    "## or by declaring the number of layers and the size of each layer:\n",
    "num_layers = 3\n",
    "hidden_size = 128 \n",
    "\n",
    "dropout = 0 # Dropout probability, default 0\n",
    "use_batchnorm = False # Whether to use batch normalization, default False\n",
    "final_activation = \"identity\" # Activation function for output layer, default nn.Identity()\n",
    "\n",
    "model = DenseNN(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    hidden_dims=hidden_dims, ## if hidden_dims is specified, num_layers and hidden_size are ignored\n",
    "    # num_layers=num_layers,\n",
    "    # hidden_size=hidden_size,\n",
    "    activation=activation,\n",
    "    dtype=dtype,\n",
    "    dropout=dropout,\n",
    "    use_batchnorm=use_batchnorm,\n",
    "    final_activation=final_activation,\n",
    ")\n",
    "\n",
    "batch_size = 5\n",
    "input = torch.randn(batch_size, input_dim, dtype=dtype)\n",
    "output = model(input)\n",
    "\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Input shape (batch_size, input_dim): {input.shape} -> Output shape (batch_size, output_dim): {output.shape}\")\n",
    "summary(model, input_data=input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef7e82f",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636de45f",
   "metadata": {},
   "source": [
    "## CNN2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e3d80",
   "metadata": {},
   "source": [
    "This model is a fully convolutional CNN similar to a U-net architecture. It is called `CNN2d` because its intended for working with 2D images, but the input data size is actually 3D as it assumes there will be a channel dimension, so the data shape is `(channels, height, width)`. For greyscale/single-channel images, the shape of a single input will be `(1, height, width)`. \n",
    "\n",
    "- 3D input data -> 3D output data\n",
    "    - `(BATCH_SIZE, channels, height, width)` -> `(BATCH_SIZE, channels, height, width)`\n",
    "    \n",
    "One limitation of CNNs is that, along the convolutional dimensions, the input size must be divisible by `2**num_layers`, i.e. for `num_layers=3`, an input size of `(204, 230)` would fail, but `(200, 232)` would work. \n",
    "\n",
    "For this and other models in this notebook, we specify all of the arguments for demonstration, but in most cases the specifics beyond input/output dimensionality can be left with the sensible defaults. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9dd3157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: <class 'quantem.core.ml.cnn.CNN2d'>\n",
      "Input shape (batch_size, channels, height, width): torch.Size([5, 1, 256, 256])\n",
      "-> Output shape (batch_size, channels, height, width): torch.Size([5, 2, 256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNN2d                                    [5, 2, 256, 256]          --\n",
       "├─ModuleList: 1-5                        --                        (recursive)\n",
       "│    └─Conv2dBlock: 2-1                  [5, 16, 256, 256]         --\n",
       "│    │    └─Sequential: 3-1              [5, 16, 256, 256]         2,480\n",
       "├─MaxPool2d: 1-2                         [5, 16, 128, 128]         --\n",
       "├─ModuleList: 1-5                        --                        (recursive)\n",
       "│    └─Conv2dBlock: 2-2                  [5, 32, 128, 128]         --\n",
       "│    │    └─Sequential: 3-2              [5, 32, 128, 128]         13,888\n",
       "├─MaxPool2d: 1-4                         [5, 32, 64, 64]           --\n",
       "├─ModuleList: 1-5                        --                        (recursive)\n",
       "│    └─Conv2dBlock: 2-3                  [5, 64, 64, 64]           --\n",
       "│    │    └─Sequential: 3-3              [5, 64, 64, 64]           55,424\n",
       "├─MaxPool2d: 1-6                         [5, 64, 32, 32]           --\n",
       "├─Conv2dBlock: 1-7                       [5, 128, 32, 32]          --\n",
       "│    └─Sequential: 2-4                   [5, 128, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-4                  [5, 128, 32, 32]          73,856\n",
       "│    │    └─ReLU: 3-5                    [5, 128, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-6                  [5, 128, 32, 32]          147,584\n",
       "│    │    └─ReLU: 3-7                    [5, 128, 32, 32]          --\n",
       "├─ModuleList: 1-12                       --                        (recursive)\n",
       "│    └─Upsample2dBlock: 2-5              [5, 64, 64, 64]           128\n",
       "│    │    └─ConvTranspose2d: 3-8         [5, 128, 64, 64]          147,584\n",
       "│    │    └─Conv2d: 3-9                  [5, 64, 64, 64]           8,256\n",
       "├─ModuleList: 1-13                       --                        (recursive)\n",
       "│    └─Conv2dBlock: 2-6                  [5, 64, 64, 64]           --\n",
       "│    │    └─Sequential: 3-10             [5, 64, 64, 64]           110,720\n",
       "├─ModuleList: 1-12                       --                        (recursive)\n",
       "│    └─Upsample2dBlock: 2-7              [5, 32, 128, 128]         64\n",
       "│    │    └─ConvTranspose2d: 3-11        [5, 64, 128, 128]         36,928\n",
       "│    │    └─Conv2d: 3-12                 [5, 32, 128, 128]         2,080\n",
       "├─ModuleList: 1-13                       --                        (recursive)\n",
       "│    └─Conv2dBlock: 2-8                  [5, 32, 128, 128]         --\n",
       "│    │    └─Sequential: 3-13             [5, 32, 128, 128]         27,712\n",
       "├─ModuleList: 1-12                       --                        (recursive)\n",
       "│    └─Upsample2dBlock: 2-9              [5, 16, 256, 256]         32\n",
       "│    │    └─ConvTranspose2d: 3-14        [5, 32, 256, 256]         9,248\n",
       "│    │    └─Conv2d: 3-15                 [5, 16, 256, 256]         528\n",
       "├─ModuleList: 1-13                       --                        (recursive)\n",
       "│    └─Conv2dBlock: 2-10                 [5, 16, 256, 256]         --\n",
       "│    │    └─Sequential: 3-16             [5, 16, 256, 256]         6,944\n",
       "├─Conv2dBlock: 1-14                      [5, 2, 256, 256]          --\n",
       "│    └─Sequential: 2-11                  [5, 2, 256, 256]          --\n",
       "│    │    └─Conv2d: 3-17                 [5, 2, 256, 256]          290\n",
       "│    │    └─Identity: 3-18               [5, 2, 256, 256]          --\n",
       "==========================================================================================\n",
       "Total params: 643,746\n",
       "Trainable params: 643,746\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 20.72\n",
       "==========================================================================================\n",
       "Input size (MB): 1.31\n",
       "Forward/backward pass size (MB): 529.53\n",
       "Params size (MB): 2.57\n",
       "Estimated Total Size (MB): 533.42\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 1 # 1 for greyscale, 3 for RGB\n",
    "out_channels = 2 # None for same number of output channels as input\n",
    "start_filters = 16 # Number of filters in first layer, doubles for each layer\n",
    "num_layers = 3 \n",
    "num_per_layer = 2 # Number of convolutional blocks per layer\n",
    "use_skip_connections = True # Whether to use skip connections (True by default)\n",
    "dtype = torch.float32 # summary looks a little odd for complex dtypes, but the model works fine\n",
    "dropout = 0 # Dropout probability, default 0\n",
    "use_batchnorm = False # Whether to use batch normalization, default False\n",
    "final_activation = \"identity\" # Activation function for output layer, default nn.Identity()\n",
    "\n",
    "model = CNN2d(\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    start_filters=start_filters,\n",
    "    num_layers=num_layers,\n",
    "    num_per_layer=num_per_layer,\n",
    "    use_skip_connections=use_skip_connections,\n",
    "    dtype=dtype,\n",
    "    dropout=dropout,\n",
    "    use_batchnorm=use_batchnorm,\n",
    "    final_activation=final_activation,\n",
    ")\n",
    "\n",
    "batch_size = 5\n",
    "image_shape = (256, 256)\n",
    "input = torch.randn(batch_size, in_channels, *image_shape, dtype=dtype)\n",
    "output = model(input)\n",
    "\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Input shape (batch_size, channels, height, width): {input.shape}\")\n",
    "print(f\"-> Output shape (batch_size, channels, height, width): {output.shape}\")\n",
    "\n",
    "summary(model, input_data=input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdd38be",
   "metadata": {},
   "source": [
    "## CNN3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1aec57",
   "metadata": {},
   "source": [
    "Basically the same as `CNN2d` but for an additional input dimension. Now the input/output data is of shape `(channels, depth, height, width)`. Note that now the depth input dimension must also be divisible by `2**num_layers`. \n",
    "\n",
    "- 4D input data -> 4D output data\n",
    "    - `(BATCH_SIZE, channels, depth, height, width)` -> `(BATCH_SIZE, channels, depth, height, width)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cf593e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: <class 'quantem.core.ml.cnn.CNN3d'>\n",
      "Input shape (batch_size, channels, depth, height, width): torch.Size([5, 1, 8, 256, 256])\n",
      "-> Output shape (batch_size, channels, depth, height, width): torch.Size([5, 2, 8, 256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNN3d                                    [5, 2, 8, 256, 256]       --\n",
       "├─ModuleList: 1-5                        --                        (recursive)\n",
       "│    └─Conv3dBlock: 2-1                  [5, 16, 8, 256, 256]      --\n",
       "│    │    └─Sequential: 3-1              [5, 16, 8, 256, 256]      7,376\n",
       "├─MaxPool3d: 1-2                         [5, 16, 4, 128, 128]      --\n",
       "├─ModuleList: 1-5                        --                        (recursive)\n",
       "│    └─Conv3dBlock: 2-2                  [5, 32, 4, 128, 128]      --\n",
       "│    │    └─Sequential: 3-2              [5, 32, 4, 128, 128]      41,536\n",
       "├─MaxPool3d: 1-4                         [5, 32, 2, 64, 64]        --\n",
       "├─ModuleList: 1-5                        --                        (recursive)\n",
       "│    └─Conv3dBlock: 2-3                  [5, 64, 2, 64, 64]        --\n",
       "│    │    └─Sequential: 3-3              [5, 64, 2, 64, 64]        166,016\n",
       "├─MaxPool3d: 1-6                         [5, 64, 1, 32, 32]        --\n",
       "├─Conv3dBlock: 1-7                       [5, 128, 1, 32, 32]       --\n",
       "│    └─Sequential: 2-4                   [5, 128, 1, 32, 32]       --\n",
       "│    │    └─Conv3d: 3-4                  [5, 128, 1, 32, 32]       221,312\n",
       "│    │    └─ReLU: 3-5                    [5, 128, 1, 32, 32]       --\n",
       "│    │    └─Conv3d: 3-6                  [5, 128, 1, 32, 32]       442,496\n",
       "│    │    └─ReLU: 3-7                    [5, 128, 1, 32, 32]       --\n",
       "├─ModuleList: 1-12                       --                        (recursive)\n",
       "│    └─Upsample3dBlock: 2-5              [5, 64, 2, 64, 64]        128\n",
       "│    │    └─ConvTranspose3d: 3-8         [5, 128, 2, 64, 64]       442,496\n",
       "│    │    └─Conv3d: 3-9                  [5, 64, 2, 64, 64]        8,256\n",
       "├─ModuleList: 1-13                       --                        (recursive)\n",
       "│    └─Conv3dBlock: 2-6                  [5, 64, 2, 64, 64]        --\n",
       "│    │    └─Sequential: 3-10             [5, 64, 2, 64, 64]        331,904\n",
       "├─ModuleList: 1-12                       --                        (recursive)\n",
       "│    └─Upsample3dBlock: 2-7              [5, 32, 4, 128, 128]      64\n",
       "│    │    └─ConvTranspose3d: 3-11        [5, 64, 4, 128, 128]      110,656\n",
       "│    │    └─Conv3d: 3-12                 [5, 32, 4, 128, 128]      2,080\n",
       "├─ModuleList: 1-13                       --                        (recursive)\n",
       "│    └─Conv3dBlock: 2-8                  [5, 32, 4, 128, 128]      --\n",
       "│    │    └─Sequential: 3-13             [5, 32, 4, 128, 128]      83,008\n",
       "├─ModuleList: 1-12                       --                        (recursive)\n",
       "│    └─Upsample3dBlock: 2-9              [5, 16, 8, 256, 256]      32\n",
       "│    │    └─ConvTranspose3d: 3-14        [5, 32, 8, 256, 256]      27,680\n",
       "│    │    └─Conv3d: 3-15                 [5, 16, 8, 256, 256]      528\n",
       "├─ModuleList: 1-13                       --                        (recursive)\n",
       "│    └─Conv3dBlock: 2-10                 [5, 16, 8, 256, 256]      --\n",
       "│    │    └─Sequential: 3-16             [5, 16, 8, 256, 256]      20,768\n",
       "├─Conv3dBlock: 1-14                      [5, 2, 8, 256, 256]       --\n",
       "│    └─Sequential: 2-11                  [5, 2, 8, 256, 256]       --\n",
       "│    │    └─Conv3d: 3-17                 [5, 2, 8, 256, 256]       866\n",
       "│    │    └─Identity: 3-18               [5, 2, 8, 256, 256]       --\n",
       "==========================================================================================\n",
       "Total params: 1,907,202\n",
       "Trainable params: 1,907,202\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 270.00\n",
       "==========================================================================================\n",
       "Input size (MB): 10.49\n",
       "Forward/backward pass size (MB): 3135.24\n",
       "Params size (MB): 7.63\n",
       "Estimated Total Size (MB): 3153.36\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 1 # 1 for greyscale, 3 for RGB\n",
    "out_channels = 2 # None for same number of output channels as input\n",
    "start_filters = 16 # Number of filters in first layer, doubles for each layer\n",
    "num_layers = 3 \n",
    "num_per_layer = 2 # Number of convolutional blocks per layer\n",
    "use_skip_connections = True # Whether to use skip connections (True by default)\n",
    "dtype = torch.float32 # summary looks a little odd for complex dtypes, but the model works fine\n",
    "dropout = 0 # Dropout probability, default 0\n",
    "use_batchnorm = False # Whether to use batch normalization, default False\n",
    "final_activation = \"identity\" # Activation function for output layer, default nn.Identity()\n",
    "\n",
    "model = CNN3d(\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    start_filters=start_filters,\n",
    "    num_layers=num_layers,\n",
    "    num_per_layer=num_per_layer,\n",
    "    use_skip_connections=use_skip_connections,\n",
    "    dtype=dtype,\n",
    "    dropout=dropout,\n",
    "    use_batchnorm=use_batchnorm,\n",
    "    final_activation=final_activation,\n",
    ")\n",
    "\n",
    "batch_size = 5\n",
    "image_shape = (8, 256, 256)\n",
    "input = torch.randn(batch_size, in_channels, *image_shape, dtype=dtype)\n",
    "output = model(input)\n",
    "\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Input shape (batch_size, channels, depth, height, width): {input.shape}\")\n",
    "print(f\"-> Output shape (batch_size, channels, depth, height, width): {output.shape}\")\n",
    "\n",
    "summary(model, input_data=input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129277dd",
   "metadata": {},
   "source": [
    "## CNN -> Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1526a366",
   "metadata": {},
   "source": [
    "3D input `(BATCH_SIZE, channels, height, width)` -> 1D output `(BATCH_SIZE, output_dim)`\n",
    "\n",
    "Unlike for a fully convolutional CNN (as demo'd above), the model will only work for a single image size, which must be set at initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "159b816c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: <class 'quantem.core.ml.cnn_dense.CNNDense'>\n",
      "Input shape (batch_size, channels, height, width): torch.Size([5, 1, 128, 128])\n",
      "-> Output shape (batch_size, output_dim): torch.Size([5, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNNDense                                 [5, 10]                   --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─Conv2dBlock: 2-1                  [5, 16, 128, 128]         --\n",
       "│    │    └─Sequential: 3-1              [5, 16, 128, 128]         2,480\n",
       "├─MaxPool2d: 1-2                         [5, 16, 64, 64]           --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─Conv2dBlock: 2-2                  [5, 32, 64, 64]           --\n",
       "│    │    └─Sequential: 3-2              [5, 32, 64, 64]           13,888\n",
       "├─MaxPool2d: 1-4                         [5, 32, 32, 32]           --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─Conv2dBlock: 2-3                  [5, 64, 32, 32]           --\n",
       "│    │    └─Sequential: 3-3              [5, 64, 32, 32]           55,424\n",
       "├─MaxPool2d: 1-6                         [5, 64, 16, 16]           --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─Conv2dBlock: 2-4                  [5, 128, 16, 16]          --\n",
       "│    │    └─Sequential: 3-4              [5, 128, 16, 16]          221,440\n",
       "├─MaxPool2d: 1-8                         [5, 128, 8, 8]            --\n",
       "├─Flatten: 1-9                           [5, 8192]                 --\n",
       "├─DenseNN: 1-10                          [5, 10]                   --\n",
       "│    └─ModuleList: 2-5                   --                        --\n",
       "│    │    └─Sequential: 3-5              [5, 128]                  1,048,704\n",
       "│    │    └─Sequential: 3-6              [5, 128]                  16,512\n",
       "│    │    └─Linear: 3-7                  [5, 10]                   1,290\n",
       "│    └─Identity: 2-6                     [5, 10]                   --\n",
       "==========================================================================================\n",
       "Total params: 1,359,738\n",
       "Trainable params: 1,359,738\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.06\n",
       "==========================================================================================\n",
       "Input size (MB): 0.33\n",
       "Forward/backward pass size (MB): 39.33\n",
       "Params size (MB): 5.44\n",
       "Estimated Total Size (MB): 45.10\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 1 # 1 for greyscale, 3 for RGB\n",
    "output_dim = 10\n",
    "image_shape = (128, 128) \n",
    "start_filters = 16 # Number of filters in first layer, doubles for each layer\n",
    "cnn_num_layers = 4\n",
    "cnn_num_per_layer = 2 # Number of convolutional blocks per layer\n",
    "dense_num_layers = 2\n",
    "dense_hidden_size = 128 # could instead specify dense_hidden_dims, a list of layer sizes\n",
    "dtype = torch.float32 \n",
    "dropout = 0 # Dropout probability, default 0\n",
    "activation = 'relu'\n",
    "final_activation = \"identity\" # Activation function for output layer, default nn.Identity()\n",
    "use_batchnorm = False # Whether to use batch normalization, default False\n",
    "\n",
    "model = CNNDense(\n",
    "    in_channels=in_channels,\n",
    "    output_dim = output_dim,\n",
    "    image_shape = image_shape,\n",
    "    start_filters = start_filters,\n",
    "    cnn_num_layers = cnn_num_layers,\n",
    "    cnn_num_per_layer = cnn_num_per_layer,\n",
    "    dense_num_layers = dense_num_layers,\n",
    "    dense_hidden_size = dense_hidden_size,\n",
    "    dtype = dtype,\n",
    "    dropout = dropout,\n",
    "    activation = activation,\n",
    "    final_activation = final_activation,\n",
    "    use_batchnorm = use_batchnorm,\n",
    ")\n",
    "\n",
    "batch_size = 5\n",
    "input = torch.randn(batch_size, in_channels, *image_shape, dtype=dtype)\n",
    "output = model(input)\n",
    "\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Input shape (batch_size, channels, height, width): {input.shape}\")\n",
    "print(f\"-> Output shape (batch_size, output_dim): {output.shape}\")\n",
    "\n",
    "summary(model, input_data=input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a8cc9",
   "metadata": {},
   "source": [
    "# Implicit Neural Representation networks (INR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0463f9ed",
   "metadata": {},
   "source": [
    "A neural network that learns to represent a continuous function by mapping coordinates to values.\n",
    "\n",
    "- Input: Coordinates `(N, in_features)` → Output: Values at those coordinates `(N, out_features)`\n",
    "\n",
    "- We primarily use `HSiren` networks, but there are many types of INRs. \n",
    "- INR networks are basically dense networks with special activation functions\n",
    "    - `Siren` type networks use sine activations throughout\n",
    "    - `HSiren` uses hyperbolic sine (`sinh`) in the first layer - better for complex-valued data and certain function classes\n",
    "\n",
    "- `HSiren` has a simple architecture, but it also includes a couple of additional parameters that typically do not need to be altered but which can be tuned for optimal performance:\n",
    "    - `first_omega_0`, `hidden_omega_0`: Control frequency of sine activations (higher = more high-frequency details)\n",
    "    - `alpha`: Weight initialization scaling factor (affects training dynamics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8470909f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: <class 'quantem.core.ml.inr.HSiren'>\n",
      "Input shape (N, in_features): torch.Size([1331, 3])\n",
      "-> Output shape (N, out_features): torch.Size([1331, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "HSiren                                   [1331, 1]                 --\n",
       "├─Sequential: 1-1                        [1331, 1]                 --\n",
       "│    └─SineLayer: 2-1                    [1331, 256]               --\n",
       "│    │    └─Linear: 3-1                  [1331, 256]               1,024\n",
       "│    └─SineLayer: 2-2                    [1331, 256]               --\n",
       "│    │    └─Linear: 3-2                  [1331, 256]               65,792\n",
       "│    └─SineLayer: 2-3                    [1331, 256]               --\n",
       "│    │    └─Linear: 3-3                  [1331, 256]               65,792\n",
       "│    └─SineLayer: 2-4                    [1331, 256]               --\n",
       "│    │    └─Linear: 3-4                  [1331, 256]               65,792\n",
       "│    └─Linear: 2-5                       [1331, 1]                 257\n",
       "│    └─Identity: 2-6                     [1331, 1]                 --\n",
       "==========================================================================================\n",
       "Total params: 198,657\n",
       "Trainable params: 198,657\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 264.41\n",
       "==========================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 21.83\n",
       "Params size (MB): 1.59\n",
       "Estimated Total Size (MB): 23.45\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_features = 3  # Dimensionality of input coordinates (3 for 3D: z, y, x; 2 for 2D: y, x)\n",
    "out_features = 1  # Dimensionality of output (1 for scalar field, 3 for vector field, etc.)\n",
    "hidden_layers = 3  # Number of hidden layers\n",
    "hidden_features = 256  # Number of features in each hidden layer\n",
    "first_omega_0 = 30.0  # Frequency scaling for first layer (default: 30)\n",
    "hidden_omega_0 = 30.0  # Frequency scaling for hidden layers (default: 30)\n",
    "alpha = 1.0  # Weight initialization scaling factor (default: 1.0)\n",
    "dtype = torch.complex64  # Can be torch.float32 or torch.complex64\n",
    "final_activation = \"identity\"  # Final activation function\n",
    "\n",
    "model = HSiren(\n",
    "    in_features=in_features,\n",
    "    out_features=out_features,\n",
    "    hidden_layers=hidden_layers,\n",
    "    hidden_features=hidden_features,\n",
    "    first_omega_0=first_omega_0,\n",
    "    hidden_omega_0=hidden_omega_0,\n",
    "    alpha=alpha,\n",
    "    dtype=dtype,\n",
    "    final_activation=final_activation,\n",
    ")\n",
    "\n",
    "# Helper method for creating an equispaced grid of coordinates\n",
    "# The grid dimensionality must match in_features\n",
    "bounds = ((0, 1), (0, 1), (0, 1))  # Volume bounds (min, max) for each dimension\n",
    "sampling = (0.1, 0.1, 0.1)  # Sampling interval for each dimension\n",
    "input = model.make_equispaced_grid(bounds, sampling)  # Returns shape (N, in_features)\n",
    "\n",
    "# The model can be evaluated at any arbitrary coordinates\n",
    "output = model(input)\n",
    "\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Input shape (N, in_features): {input.shape}\")\n",
    "print(f\"-> Output shape (N, out_features): {output.shape}\")\n",
    "\n",
    "summary(model, input_data=input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49192d7",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279318c",
   "metadata": {},
   "source": [
    "Convolutional autoencoder for dimensionality reduction and feature extraction from images. The model encodes an image into a lower-dimensional latent representation, then decodes it back to reconstruct the original image.\n",
    "\n",
    "- 3D input `(BATCH_SIZE, channels, height, width)` -> 1D latent `(BATCH_SIZE, latent_dim)` -> 3D output `(BATCH_SIZE, channels, height, width)`\n",
    "\n",
    "- Latent normalization options: Control the structure of the latent space for different clustering algorithms\n",
    "  - `\"none\"`: Raw latent vectors\n",
    "  - `\"l2\"`: Unit hypersphere (good for DBSCAN/K-means)\n",
    "  - `\"layer\"`: Gaussian-like distribution (good for GMM)\n",
    "  - `\"tanh\"`: Bounded in [-1, 1]\n",
    "  \n",
    "- Forward pass returns both the output and the latent space representation: `output, latent = model(input)`\n",
    "- Can encode/decode separately: \n",
    "    - `latent = model.encode(input)` \n",
    "    - `output = model.decode(latent)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84631df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: <class 'quantem.core.ml.conv_autoencoder.ConvAutoencoder2d'>\n",
      "Input shape (batch_size, channels, height, width): torch.Size([5, 1, 256, 256])\n",
      "-> latent shape (batch_size, latent_dim): torch.Size([5, 64])\n",
      "-> Output shape (batch_size, channels, height, width): torch.Size([5, 1, 256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ConvAutoencoder2d                        [5, 1, 256, 256]          --\n",
       "├─ModuleList: 1-5                        --                        (recursive)\n",
       "│    └─Conv2dBlock: 2-1                  [5, 16, 256, 256]         --\n",
       "│    │    └─Sequential: 3-1              [5, 16, 256, 256]         2,544\n",
       "├─MaxPool2d: 1-2                         [5, 16, 128, 128]         --\n",
       "├─ModuleList: 1-5                        --                        (recursive)\n",
       "│    └─Conv2dBlock: 2-2                  [5, 32, 128, 128]         --\n",
       "│    │    └─Sequential: 3-2              [5, 32, 128, 128]         14,016\n",
       "├─MaxPool2d: 1-4                         [5, 32, 64, 64]           --\n",
       "├─ModuleList: 1-5                        --                        (recursive)\n",
       "│    └─Conv2dBlock: 2-3                  [5, 64, 64, 64]           --\n",
       "│    │    └─Sequential: 3-3              [5, 64, 64, 64]           55,680\n",
       "├─MaxPool2d: 1-6                         [5, 64, 32, 32]           --\n",
       "├─Linear: 1-7                            [5, 64]                   4,194,368\n",
       "├─Linear: 1-8                            [5, 65536]                4,259,840\n",
       "├─ModuleList: 1-13                       --                        (recursive)\n",
       "│    └─Upsample2dBlock: 2-4              [5, 32, 64, 64]           --\n",
       "│    │    └─ConvTranspose2d: 3-4         [5, 64, 64, 64]           36,928\n",
       "│    │    └─Conv2d: 3-5                  [5, 32, 64, 64]           2,080\n",
       "│    │    └─BatchNorm2d: 3-6             [5, 32, 64, 64]           64\n",
       "├─ModuleList: 1-14                       --                        (recursive)\n",
       "│    └─Conv2dBlock: 2-5                  [5, 32, 64, 64]           --\n",
       "│    │    └─Sequential: 3-7              [5, 32, 64, 64]           18,624\n",
       "├─ModuleList: 1-13                       --                        (recursive)\n",
       "│    └─Upsample2dBlock: 2-6              [5, 16, 128, 128]         --\n",
       "│    │    └─ConvTranspose2d: 3-8         [5, 32, 128, 128]         9,248\n",
       "│    │    └─Conv2d: 3-9                  [5, 16, 128, 128]         528\n",
       "│    │    └─BatchNorm2d: 3-10            [5, 16, 128, 128]         32\n",
       "├─ModuleList: 1-14                       --                        (recursive)\n",
       "│    └─Conv2dBlock: 2-7                  [5, 16, 128, 128]         --\n",
       "│    │    └─Sequential: 3-11             [5, 16, 128, 128]         4,704\n",
       "├─ModuleList: 1-13                       --                        (recursive)\n",
       "│    └─Upsample2dBlock: 2-8              [5, 1, 256, 256]          2\n",
       "│    │    └─ConvTranspose2d: 3-12        [5, 16, 256, 256]         2,320\n",
       "│    │    └─Conv2d: 3-13                 [5, 1, 256, 256]          17\n",
       "├─ModuleList: 1-14                       --                        (recursive)\n",
       "│    └─Conv2dBlock: 2-9                  [5, 1, 256, 256]          --\n",
       "│    │    └─Sequential: 3-14             [5, 1, 256, 256]          20\n",
       "==========================================================================================\n",
       "Total params: 8,601,015\n",
       "Trainable params: 8,601,015\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 6.26\n",
       "==========================================================================================\n",
       "Input size (MB): 1.31\n",
       "Forward/backward pass size (MB): 471.86\n",
       "Params size (MB): 34.40\n",
       "Estimated Total Size (MB): 507.58\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_channels = 1  # 1 for a greyscale image, 3 for RGB, 4 for RGBA, etc.\n",
    "image_shape = (256, 256)  # shape of the image input (same as output) (height, width)\n",
    "shape = (input_channels, *image_shape)\n",
    "start_filters = 16\n",
    "latent_dim = 64\n",
    "latent_normalization = \"l2\" # \"none\", \"l2\", \"layer\", \"tanh\"\n",
    "num_layers = 3\n",
    "num_per_layer = 2\n",
    "dtype = torch.float32\n",
    "activation = \"relu\"\n",
    "final_activation = \"relu\"\n",
    "\n",
    "\n",
    "model = ConvAutoencoder2d(\n",
    "    input_size=image_shape,\n",
    "    start_filters=start_filters,\n",
    "    latent_dim=latent_dim,\n",
    "    latent_normalization=latent_normalization,\n",
    "    num_layers=num_layers,\n",
    "    num_per_layer=num_per_layer,\n",
    "    dtype=dtype,\n",
    "    activation=activation,\n",
    "    final_activation=final_activation,\n",
    ")\n",
    "\n",
    "batch_size = 5\n",
    "input_shape = (batch_size, *shape)\n",
    "input = torch.rand(*input_shape, dtype=dtype)\n",
    "\n",
    "output, latent_repr = model.forward(input)\n",
    "## model.forward is equivalent to:\n",
    "## latent = model.encode(input)\n",
    "## output = model.decode(latent)\n",
    "## return output, latent\n",
    "\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Input shape (batch_size, channels, height, width): {input.shape}\")\n",
    "print(f\"-> latent shape (batch_size, latent_dim): {latent_repr.shape}\")\n",
    "print(f\"-> Output shape (batch_size, channels, height, width): {output.shape}\")\n",
    "\n",
    "summary(model, input_data=input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47640f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
